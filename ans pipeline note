### Input: article, question (list)

### Passage retrieval: find top N sentences with relavence score

### Answer generation:

#### Question classification:
##### wh- Questions:

if Q starts with "who": [PERSON]

if Q starts with "where": [LOC]

if Q starts with "when": [DATE]

if Q stars with "what": 

find the headword after the "what" question word

- What was **reached** not at Saqqara?

if headword has NER, then decide which question type it is


##### binary Questions:

if Q starts with "is", "are", "was", "were", "does", "do", "did", "have", "has", "had", "will", "would", "could"

##### other questions:
if Q doesn't fall into any of above category (DESCRIPTION/REASON):


#### Answer types:
[PERSON]:
1. Look for phrases with the NER tags: [PERSON][ORG] in the text that contains answer.
2. only look for phrases where proper noun is the **subject** within sentence.
3. Process the Question to formalize the Answer.

[DATE]:
1. Look for phrases with the NER tags: [DATE][TIME][CARDINAL] from the text that contains answer.
2. Use dependency parsing to formalize the answer.

[LOC]:
1. Look for phrases with the NER tags: [GPE][LOC], in the text that contains answer.
2. if more than one correct NER in the candidate text:
  - text = "The Old Kingdom is perhaps best known for the large number of pyramids constructed at this time as burial places for Egypt's kings"
  - Q = "What is perhaps best known for the large number of pyramids constructed at this time as burial places for Egypt's kings?"
  {'Old Kingdom': 'GPE', 'Egypt': 'GPE'}
  **novelty rule**: ans should be the one that doesn't appear in the Q
  - ans = "Old Kingdom"

[BINARY]:
Do the sentiment analysis for the top n sentences of candidate answer

[DESCRIPTION][REASON]:
reconstruct the candidate answer text to make it grammarly correct
